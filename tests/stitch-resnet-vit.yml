#
# We're stitching a small-ish ResNet and a ViT with similar performance metrics.
#  - resnet34.a2_in1k:                          75.5% (ResNet Strikes Back A2 recipe)
#  - vit_tiny_patch16_224.augreg_in21k_ft_in1k: 75.5% (Self-supervised + fine-tuning from "How to train your ViT")
#

# CNN --> ViT architecture
assembly:
  - Subnet:
      backend: timm
      model_name: resnet34.a2_in1k
      block_input: x
      block_output: layer2
      in_format: img
      out_format: img
      frozen: true
  - SimpleAdapter:
      num_conv: 1
      in_channels: 128
      out_channels: 192
  - Subnet:
      backend: timm
      model_name: vit_tiny_patch16_224.augreg_in21k_ft_in1k
      block_input: blocks.6
      block_output: head
      in_format: [bert, 196]
      out_format: vector
      frozen: true

# ViT --> CNN architecture
#assembly:
#  - Subnet:
#      backend: timm
#      model_name: vit_tiny_patch16_224.augreg_in21k_ft_in1k
#      block_input: x
#      block_output: blocks.5
#      in_format: img
#      out_format: bert
#      frozen: true
#  - SimpleAdapter:
#      num_conv: 1
#      in_channels: 384  # 192 * 2 to account for the output token
#      out_channels: 128
#  - Subnet:
#      backend: timm
#      model_name: resnet34.a2_in1k
#      block_input: layer3
#      block_output: fc
#      in_format: [img, [28, 28]]
#      out_format: vector
#      frozen: true


train_config:
  # Dataset
  dataset: imagenet
  data_augmentation: true

  # General Params
  seed: 12345
  epochs: &epochs 10

  # Optimization
  # In Appendix A.4 of "Revisiting Model Stitching" they say, "All stitching layers were optimized with Adam cosine
  # learning rate schedule and initial learning rate 0.001".
  batch_size: 256
  optimizer: AdamW
  optimizer_args:
    lr: 2.0e-3
    weight_decay: 0.05
  lr_scheduler: CosineAnnealingLR
  lr_scheduler_args:
    T_max: *epochs
    eta_min: 0.0
