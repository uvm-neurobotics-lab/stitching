#
# Train ResNet-50 from scratch.
#
# For reference, potential results of different training recipes:
#  - resnet50.fb_swsl_ig1b_ft_in1k:    81.1% (Pretrained on Instagram-1B using SWSL, then fine-tuned on IN-1k)
#  - resnet50.tv2_in1k:                80.8% (Updated Torchvision recipe)
#  - resnet50.a1h_in1k:                80.7% (Based on A1 but with stronger dropout, stochastic depth, and RandAugment)
#  - resnet50.a1_in1k:                 80.4% (ResNet Strikes Back A1 recipe; 600 epochs)
#  - resnet50.d_in1k:                  79.9% (Recipe D from Appendix of ResNet Strikes Back)
#  - resnet50.a2_in1k:                 79.8% (ResNet Strikes Back A2 recipe; 300 epochs)
#  - resnet50.c1_in1k:                 79.8% (Recipe C.1 from Appendix of ResNet Strikes Back)
#  - resnet50.c2_in1k:                 79.9% (Recipe C.2 from Appendix of ResNet Strikes Back)
#  - resnet50.b1k_in1k:                79.6% (Recipe B from Appendix of ResNet Strikes Back)
#  - resnet50.b2k_in1k:                79.4% (Recipe B from Appendix of ResNet Strikes Back)
#  - resnet50.fb_ssl_yfcc100m_ft_in1k: 79.3% (Pretrained on a subset of YFCC100M using SSL)
#  - resnet50.ram_in1k:                79.0% (SGD+momentum, CosineAnnealing, AugMix + RandAugment recipe)
#  - resnet50.am_in1k:                 79.0% (SGD+momentum, CosineAnnealing, AugMix Recipe)
#  - resnet50.ra_in1k:                 78.8% (Equivalent to Recipe B)
#  - resnet50.bt_in1k:                 78.4% (Bag-of-Tricks recipe)
#  - resnet50.a3_in1k:                 78.1% (ResNet Strikes Back A3 recipe; 100 epochs)
#  - resnet50.gluon_in1k:              77.6% (Bag-of-Tricks recipe)
#  - resnet50.tv_in1k:                 76.2% (Original Torchvision recipe)
#

# Architecture to assemble. A series of blocks with adapters in between each.
assembly:
  - Net:
      backend: timm
      model_name: resnet50
      pretrained: false

save_checkpoints: true

train_config:
  # Dataset
  dataset: imagenet
  data_augmentation: true

  # General Params
  seed: 12345
  epochs: 90

  # Optimization
  # This plus default data augmentation should approximate the recipe found in Torchvision:
  #     https://github.com/pytorch/vision/tree/main/references/classification#resnet
  batch_size: 256
  optimizer: SGD
  optimizer_args:
    lr: 0.1
    momentum: 0.9
    weight_decay: 1e-4
  lr_scheduler: StepLR
  lr_scheduler_args:
    step_size: 30
    gamma: 0.1
