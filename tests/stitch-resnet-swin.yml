#
# Use the ResNet-50/Swin-T hybrid shown in DeRy, Table 3. It achieves good accuracy on CIFAR-100, shown in the table.
# But what will it achieve on ImageNet? Will it recover the accuracy of original models?
#  - resnet50.a1_in1k:                            80.4% (ResNet Strikes Back A1 recipe)
#  - resnet50.fb_swsl_ig1b_ft_in1k (alternative): 81.1% (SWSL Self-Supervised Pre-Training)
#  - swin_t:                                      81.5% (Swin-Tiny)
#  - swin_v2_t (alternative):                     82.1% (SwinV2-Tiny)
# The paper says, "ResNet50 (Stage 1 & 2), Swin-T (Stage 3 & 4)". For PyTorch's Swin, the naming of the blocks doesn't
# quite line up with the "stages", but Stage 3 & 4 would correspond to `features.5` and `features.7`.
#

# CNN --> ViT architecture
assembly:
  - Subnet:
      backend: timm
      model_name: resnet50.a1_in1k
      block_input: x
      block_output: layer2
      in_format: img
      out_format: img
      frozen: true
  - SimpleAdapter:
      num_conv: 1
      in_channels: 512
      out_channels: 384
  - Subnet:
      backend: pytorch
      model_name: swin_t
      block_input: features.5
      block_output: head
      in_format: [bhwc, [14, 14]]
      out_format: vector
      frozen: true


train_config:
  # Dataset
  dataset: imagenet
  data_augmentation: true

  # General Params
  seed: 12345
  epochs: &epochs 10

  # Optimization
  # In Appendix A.4 of "Revisiting Model Stitching" they say, "All stitching layers were optimized with Adam cosine
  # learning rate schedule and initial learning rate 0.001".
  batch_size: 256
  optimizer: AdamW
  optimizer_args:
    lr: 2.0e-3
    weight_decay: 0.05
  lr_scheduler: CosineAnnealingLR
  lr_scheduler_args:
    T_max: *epochs
    eta_min: 0.0
