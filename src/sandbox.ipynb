{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-19T18:25:46.572062Z",
     "start_time": "2025-06-19T18:25:46.530204Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from assembly import Assembly\n",
    "from utils import load_yaml, num_params\n",
    "from utils.models import load_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-20T21:22:45.416947Z",
     "start_time": "2025-06-20T21:22:45.342351Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 cores.\n"
     ]
    }
   ],
   "source": [
    "sns.set(font_scale=1.25, style=\"whitegrid\")\n",
    "\n",
    "# Ignore known warnings that come when constructing subnets.\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The parameter 'pretrained' is deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Arguments other than a weight enum.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*already erased node.*\")\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device = {device}\")\n",
    "\n",
    "NUM_CORES = os.cpu_count()\n",
    "if hasattr(os, \"sched_getaffinity\"):\n",
    "    # This function is only available on certain platforms. When running with Slurm, it can tell us the true\n",
    "    # number of cores we have access to.\n",
    "    NUM_CORES = len(os.sched_getaffinity(0))\n",
    "print (f\"Using {NUM_CORES} cores.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-20T20:51:40.667664Z",
     "start_time": "2025-06-20T20:51:40.636305Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Counting Compute\n",
    "\n",
    "Figuring out how to count params and FLOPs, or whatever might be a good proxy for amount of compute."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "from torchtnt.utils.flops import FlopTensorDispatchMode\n",
    "\n",
    "def print_assembly_sizes(cfile):\n",
    "    cfg = load_yaml(cfile)\n",
    "    assembly = cfg.get(\"assembly\", cfg.get(\"stages\", cfg.get(\"src_stages\")))\n",
    "    model = Assembly(assembly, cfg.get(\"head\"), input_shape=[3, 224, 224])\n",
    "    print_sizes(model)\n",
    "\n",
    "\n",
    "def print_sizes(model):\n",
    "    model.train()\n",
    "    print(f\"Number of parameters: {num_params(model):.3e}\")\n",
    "    if hasattr(model, \"parts\"):\n",
    "        for i, p in enumerate(model.parts):\n",
    "            print(f\"Num params in part {i+1} ({p.__class__.__name__}): {num_params(p):.3e}\")\n",
    "\n",
    "    dummy_img = torch.randn(1, 3, 224, 224)\n",
    "    flops = FlopCountAnalysis(model, dummy_img)\n",
    "    print(f\"\\nfvcore FLOPs: {flops.total():.3e}\")\n",
    "    # print(f\"fvcore FLOPs by module: {flops.by_module()}\")\n",
    "    # print(f\"fvcore FLOPs by operator: {flops.by_operator()}\")\n",
    "    with FlopTensorDispatchMode(model) as ftdm:\n",
    "        _ = model(dummy_img)\n",
    "        print(f\"\\nTorchTNT FLOPs: {sum(ftdm.flop_counts[''].values()):.3e}\")\n",
    "        # print(f\"\\nTorchTNT FLOPs breakdown:\")\n",
    "        # for k, v in ftdm.flop_counts.items():\n",
    "        #     print(f\"    {k}: {v:.3e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-20T20:52:45.506641Z",
     "start_time": "2025-06-20T20:52:45.476774Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::hardswish_ encountered 21 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 10 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 8 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::hardsigmoid encountered 8 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 8 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5.483e+06\n",
      "Num params in part 1 (Subnet): 4.368e+03\n",
      "Num params in part 2 (Subnet): 4.440e+03\n",
      "Num params in part 3 (Subnet): 1.033e+04\n",
      "Num params in part 4 (Subnet): 4.198e+04\n",
      "Num params in part 5 (Subnet): 3.208e+04\n",
      "Num params in part 6 (Subnet): 6.993e+05\n",
      "Num params in part 7 (Subnet): 4.292e+05\n",
      "Num params in part 8 (Subnet): 4.261e+06\n",
      "\n",
      "fvcore FLOPs: 2.386e+08\n",
      "\n",
      "TorchTNT FLOPs: 2.166e+08\n"
     ]
    }
   ],
   "source": [
    "print_assembly_sizes(Path(\"../across-scales/mobilenet-v3.yml\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-20T20:53:09.995811Z",
     "start_time": "2025-06-20T20:53:06.212596Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 53 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 16 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2.556e+07\n",
      "Num params in part 1 (Subnet): 8.454e+04\n",
      "Num params in part 2 (Subnet): 1.408e+05\n",
      "Num params in part 3 (Subnet): 3.794e+05\n",
      "Num params in part 4 (Subnet): 8.402e+05\n",
      "Num params in part 5 (Subnet): 1.512e+06\n",
      "Num params in part 6 (Subnet): 5.586e+06\n",
      "Num params in part 7 (Subnet): 6.040e+06\n",
      "Num params in part 8 (Subnet): 1.097e+07\n",
      "\n",
      "fvcore FLOPs: 4.145e+09\n",
      "\n",
      "TorchTNT FLOPs: 4.089e+09\n"
     ]
    }
   ],
   "source": [
    "print_assembly_sizes(Path(\"../across-scales/resnet-50.yml\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-20T20:53:13.837805Z",
     "start_time": "2025-06-20T20:53:09.996269Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2.829e+07\n",
      "Num params in part 1 (Subnet): 4.896e+03\n",
      "Num params in part 2 (Subnet): 2.247e+05\n",
      "Num params in part 3 (Subnet): 7.450e+04\n",
      "Num params in part 4 (Subnet): 8.918e+05\n",
      "Num params in part 5 (Subnet): 2.964e+05\n",
      "Num params in part 6 (Subnet): 1.066e+07\n",
      "Num params in part 7 (Subnet): 1.183e+06\n",
      "Num params in part 8 (Subnet): 1.495e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::rsub encountered 24 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pad encountered 15 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 58 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 12 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 41 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::softmax encountered 12 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 12 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::fill_ encountered 45 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 5 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::ne encountered 5 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::bernoulli_ encountered 22 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div_ encountered 22 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "parts.1.net.features.1.0.stochastic_depth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fvcore FLOPs: 4.509e+09\n",
      "\n",
      "TorchTNT FLOPs: 4.491e+09\n"
     ]
    }
   ],
   "source": [
    "print_assembly_sizes(Path(\"../across-scales/swin-t.yml\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-20T20:53:18.975133Z",
     "start_time": "2025-06-20T20:53:13.837600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 28 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1.169e+07\n",
      "\n",
      "fvcore FLOPs: 1.827e+09\n",
      "\n",
      "TorchTNT FLOPs: 1.814e+09\n"
     ]
    }
   ],
   "source": [
    "print_sizes(load_model(\"resnet18\", \"pytorch\", pretrained=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T15:11:33.298048Z",
     "start_time": "2025-06-27T15:11:32.614164Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below: demonstrating how the number of params and FLOPs in a ResNet BottleneckBlock are actually much smaller than a single conv3x3 layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 53 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 16 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2.556e+07\n",
      "Num params in part 1 (Subnet): 1.445e+06\n",
      "Num params in part 2 (Subnet): 1.512e+06\n",
      "Num params in part 3 (Subnet): 2.260e+07\n",
      "\n",
      "fvcore FLOPs: 4.145e+09\n",
      "\n",
      "TorchTNT FLOPs: 4.089e+09\n"
     ]
    }
   ],
   "source": [
    "# Assembly equivalent to a ResNet-50.\n",
    "assembly_config = [\n",
    "    {\"Subnet\": {\n",
    "        \"backend\": \"timm\",\n",
    "        \"model_name\": \"resnet50.a1_in1k\",\n",
    "        \"block_input\": \"x\",\n",
    "        \"block_output\": \"layer2.3\",\n",
    "        \"in_format\": \"img\",  # layer1.0 input is [64, 56, 56].\n",
    "        \"out_format\": [\"img\", [512, 28, 28]],\n",
    "    }},\n",
    "    {\"Subnet\": {  # Downsample block\n",
    "        \"backend\": \"timm\",\n",
    "        \"model_name\": \"resnet50.a1_in1k\",\n",
    "        \"block_input\": \"layer3.0\",\n",
    "        \"block_output\": \"layer3.0\",\n",
    "        \"in_format\": [\"img\", [512, 28, 28]],\n",
    "        \"out_format\": [\"img\", [1024, 14, 14]],\n",
    "    }},\n",
    "    {\"Subnet\": {\n",
    "        \"backend\": \"timm\",\n",
    "        \"model_name\": \"resnet50.a1_in1k\",\n",
    "        \"block_input\": \"layer3.1\",\n",
    "        \"block_output\": \"fc\",\n",
    "        \"in_format\": [\"img\", [1024, 14, 14]],\n",
    "        \"out_format\": \"vector\",\n",
    "    }},\n",
    "]\n",
    "\n",
    "def newcfg():\n",
    "    return deepcopy(assembly_config)\n",
    "\n",
    "\n",
    "model = Assembly(newcfg(), input_shape=[3, 224, 224])\n",
    "print_sizes(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-21T00:20:22.045554Z",
     "start_time": "2025-06-21T00:20:19.881987Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "from launch_scaling_experiments import linear, stitch, conv3x3, bottleneck, stitch_no_downsample\n",
    "\n",
    "gap = {\n",
    "    \"blocks_to_drop\": [1, 1],\n",
    "    \"num_downsamples\": 1,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-21T00:20:22.078769Z",
     "start_time": "2025-06-21T00:20:22.047701Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 51 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 15 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::leaky_relu_ encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2.877e+07\n",
      "Num params in part 1 (Subnet): 1.445e+06\n",
      "Num params in part 2 (SimpleAdapter): 4.723e+06\n",
      "Num params in part 3 (Subnet): 2.260e+07\n",
      "\n",
      "fvcore FLOPs: 4.696e+09\n",
      "\n",
      "TorchTNT FLOPs: 4.642e+09\n"
     ]
    }
   ],
   "source": [
    "conv3x3_assembly = stitch(newcfg(), newcfg(), gap, conv3x3)\n",
    "model = Assembly(conv3x3_assembly, input_shape=[3, 224, 224])\n",
    "print_sizes(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-21T00:20:23.623703Z",
     "start_time": "2025-06-21T00:20:22.076999Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 54 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 15 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2.556e+07\n",
      "Num params in part 1 (Subnet): 1.445e+06\n",
      "Num params in part 2 (ResNetBottleneck): 1.512e+06\n",
      "Num params in part 3 (Subnet): 2.260e+07\n",
      "\n",
      "fvcore FLOPs: 4.067e+09\n",
      "\n",
      "TorchTNT FLOPs: 4.012e+09\n"
     ]
    }
   ],
   "source": [
    "bottleneck_assembly = stitch(newcfg(), newcfg(), gap, bottleneck)\n",
    "model = Assembly(bottleneck_assembly, input_shape=[3, 224, 224])\n",
    "print_sizes(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-21T00:20:24.880362Z",
     "start_time": "2025-06-21T00:20:23.624103Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2.556e+07\n",
      "Num params in part 1 (Subnet): 1.445e+06\n",
      "Num params in part 2 (ResNetBottleneck): 1.512e+06\n",
      "Num params in part 3 (Subnet): 2.260e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 54 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 15 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fvcore FLOPs: 1.070e+10\n",
      "\n",
      "TorchTNT FLOPs: 1.060e+10\n"
     ]
    }
   ],
   "source": [
    "bottleneck_no_downsample_assembly = stitch_no_downsample(newcfg(), newcfg(), gap, bottleneck)\n",
    "model = Assembly(bottleneck_no_downsample_assembly, input_shape=[3, 224, 224])\n",
    "print_sizes(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-21T00:20:26.218965Z",
     "start_time": "2025-06-21T00:20:24.882918Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 51 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 15 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2.457e+07\n",
      "Num params in part 1 (Subnet): 1.445e+06\n",
      "Num params in part 2 (SimpleAdapter): 5.284e+05\n",
      "Num params in part 3 (Subnet): 2.260e+07\n",
      "\n",
      "fvcore FLOPs: 3.906e+09\n",
      "\n",
      "TorchTNT FLOPs: 3.851e+09\n"
     ]
    }
   ],
   "source": [
    "linear_assembly = stitch(newcfg(), newcfg(), gap, linear)\n",
    "model = Assembly(linear_assembly, input_shape=[3, 224, 224])\n",
    "print_sizes(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-21T00:20:27.479746Z",
     "start_time": "2025-06-21T00:20:26.219539Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2.457e+07\n",
      "Num params in part 1 (Subnet): 1.445e+06\n",
      "Num params in part 2 (SimpleAdapter): 5.284e+05\n",
      "Num params in part 3 (Subnet): 2.260e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 51 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 15 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fvcore FLOPs: 1.111e+10\n",
      "\n",
      "TorchTNT FLOPs: 1.102e+10\n"
     ]
    }
   ],
   "source": [
    "linear_no_downsample_assembly = stitch_no_downsample(newcfg(), newcfg(), gap, linear)\n",
    "model = Assembly(linear_no_downsample_assembly, input_shape=[3, 224, 224])\n",
    "print_sizes(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-21T00:20:28.860318Z",
     "start_time": "2025-06-21T00:20:27.480718Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Printing Model Computation Graph\n",
    "\n",
    "A cell for inspecting the computation graph of a model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2.829e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::rsub encountered 24 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pad encountered 15 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 58 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::pow encountered 12 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 41 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::softmax encountered 12 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 12 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::fill_ encountered 45 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 5 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::ne encountered 5 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::bernoulli_ encountered 22 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div_ encountered 22 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "features.1.0.attn.proj, features.1.0.attn.qkv, features.1.0.stochastic_depth, features.1.1.attn.proj, features.1.1.attn.qkv, features.3.0.attn.proj, features.3.0.attn.qkv, features.3.1.attn.proj, features.3.1.attn.qkv, features.5.0.attn.proj, features.5.0.attn.qkv, features.5.1.attn.proj, features.5.1.attn.qkv, features.5.2.attn.proj, features.5.2.attn.qkv, features.5.3.attn.proj, features.5.3.attn.qkv, features.5.4.attn.proj, features.5.4.attn.qkv, features.5.5.attn.proj, features.5.5.attn.qkv, features.7.0.attn.proj, features.7.0.attn.qkv, features.7.1.attn.proj, features.7.1.attn.qkv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fvcore FLOPs: 4.509e+09\n",
      "\n",
      "TorchTNT FLOPs: 4.491e+09\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import timm\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import fx\n",
    "from utils import num_params\n",
    "from utils.subgraphs import _get_leaf_modules_for_ops, NodePathTracer\n",
    "\n",
    "# We could load a model either from Torchvision or timm.\n",
    "model = torchvision.models.swin_t()\n",
    "# model = timm.create_model(\"resnet50\")\n",
    "# print(model)\n",
    "tracer = NodePathTracer(autowrap_modules=(math, torchvision.ops), leaf_modules=_get_leaf_modules_for_ops())\n",
    "graph = tracer.trace(model)\n",
    "print_sizes(model)\n",
    "# print(f\"Number of parameters in graph: {num_params(fx.GraphModule(tracer.root, graph, model.__class__.__name__)):.3e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-25T13:51:30.437190Z",
     "start_time": "2025-06-25T13:51:29.177534Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %features_0_0 : [num_users=1] = call_module[target=features.0.0](args = (%x,), kwargs = {})\n",
      "    %features_0_1 : [num_users=1] = call_module[target=features.0.1](args = (%features_0_0,), kwargs = {})\n",
      "    %features_0_2 : [num_users=2] = call_module[target=features.0.2](args = (%features_0_1,), kwargs = {})\n",
      "    %features_1_0_norm1 : [num_users=1] = call_module[target=features.1.0.norm1](args = (%features_0_2,), kwargs = {})\n",
      "    %features_1_0_attn_relative_position_bias_table : [num_users=1] = get_attr[target=features.1.0.attn.relative_position_bias_table]\n",
      "    %features_1_0_attn_relative_position_index : [num_users=1] = get_attr[target=features.1.0.attn.relative_position_index]\n",
      "    %_get_relative_position_bias : [num_users=1] = call_function[target=torchvision.models.swin_transformer._get_relative_position_bias](args = (%features_1_0_attn_relative_position_bias_table, %features_1_0_attn_relative_position_index, [7, 7]), kwargs = {})\n",
      "    %features_1_0_attn_qkv_weight : [num_users=1] = get_attr[target=features.1.0.attn.qkv.weight]\n",
      "    %features_1_0_attn_proj_weight : [num_users=1] = get_attr[target=features.1.0.attn.proj.weight]\n",
      "    %features_1_0_attn_qkv_bias : [num_users=1] = get_attr[target=features.1.0.attn.qkv.bias]\n",
      "    %features_1_0_attn_proj_bias : [num_users=1] = get_attr[target=features.1.0.attn.proj.bias]\n",
      "    %shifted_window_attention : [num_users=1] = call_function[target=torchvision.models.swin_transformer.shifted_window_attention](args = (%features_1_0_norm1, %features_1_0_attn_qkv_weight, %features_1_0_attn_proj_weight, %_get_relative_position_bias, [7, 7], 3), kwargs = {shift_size: [0, 0], attention_dropout: 0.0, dropout: 0.0, qkv_bias: %features_1_0_attn_qkv_bias, proj_bias: %features_1_0_attn_proj_bias, training: True})\n",
      "    %features_1_0_stochastic_depth : [num_users=1] = call_module[target=features.1.0.stochastic_depth](args = (%shifted_window_attention,), kwargs = {})\n",
      "    %add : [num_users=2] = call_function[target=operator.add](args = (%features_0_2, %features_1_0_stochastic_depth), kwargs = {})\n",
      "    %features_1_0_norm2 : [num_users=1] = call_module[target=features.1.0.norm2](args = (%add,), kwargs = {})\n",
      "    %features_1_0_mlp : [num_users=1] = call_module[target=features.1.0.mlp](args = (%features_1_0_norm2,), kwargs = {})\n",
      "    %features_1_0_stochastic_depth_1 : [num_users=1] = call_module[target=features.1.0.stochastic_depth](args = (%features_1_0_mlp,), kwargs = {})\n",
      "    %add_1 : [num_users=2] = call_function[target=operator.add](args = (%add, %features_1_0_stochastic_depth_1), kwargs = {})\n",
      "    %features_1_1_norm1 : [num_users=1] = call_module[target=features.1.1.norm1](args = (%add_1,), kwargs = {})\n",
      "    %features_1_1_attn_relative_position_bias_table : [num_users=1] = get_attr[target=features.1.1.attn.relative_position_bias_table]\n",
      "    %features_1_1_attn_relative_position_index : [num_users=1] = get_attr[target=features.1.1.attn.relative_position_index]\n",
      "    %_get_relative_position_bias_1 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._get_relative_position_bias](args = (%features_1_1_attn_relative_position_bias_table, %features_1_1_attn_relative_position_index, [7, 7]), kwargs = {})\n",
      "    %features_1_1_attn_qkv_weight : [num_users=1] = get_attr[target=features.1.1.attn.qkv.weight]\n",
      "    %features_1_1_attn_proj_weight : [num_users=1] = get_attr[target=features.1.1.attn.proj.weight]\n",
      "    %features_1_1_attn_qkv_bias : [num_users=1] = get_attr[target=features.1.1.attn.qkv.bias]\n",
      "    %features_1_1_attn_proj_bias : [num_users=1] = get_attr[target=features.1.1.attn.proj.bias]\n",
      "    %shifted_window_attention_1 : [num_users=1] = call_function[target=torchvision.models.swin_transformer.shifted_window_attention](args = (%features_1_1_norm1, %features_1_1_attn_qkv_weight, %features_1_1_attn_proj_weight, %_get_relative_position_bias_1, [7, 7], 3), kwargs = {shift_size: [3, 3], attention_dropout: 0.0, dropout: 0.0, qkv_bias: %features_1_1_attn_qkv_bias, proj_bias: %features_1_1_attn_proj_bias, training: True})\n",
      "    %features_1_1_stochastic_depth : [num_users=1] = call_module[target=features.1.1.stochastic_depth](args = (%shifted_window_attention_1,), kwargs = {})\n",
      "    %add_2 : [num_users=2] = call_function[target=operator.add](args = (%add_1, %features_1_1_stochastic_depth), kwargs = {})\n",
      "    %features_1_1_norm2 : [num_users=1] = call_module[target=features.1.1.norm2](args = (%add_2,), kwargs = {})\n",
      "    %features_1_1_mlp : [num_users=1] = call_module[target=features.1.1.mlp](args = (%features_1_1_norm2,), kwargs = {})\n",
      "    %features_1_1_stochastic_depth_1 : [num_users=1] = call_module[target=features.1.1.stochastic_depth](args = (%features_1_1_mlp,), kwargs = {})\n",
      "    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%add_2, %features_1_1_stochastic_depth_1), kwargs = {})\n",
      "    %_patch_merging_pad : [num_users=1] = call_function[target=torchvision.models.swin_transformer._patch_merging_pad](args = (%add_3,), kwargs = {})\n",
      "    %features_2_norm : [num_users=1] = call_module[target=features.2.norm](args = (%_patch_merging_pad,), kwargs = {})\n",
      "    %features_2_reduction : [num_users=2] = call_module[target=features.2.reduction](args = (%features_2_norm,), kwargs = {})\n",
      "    %features_3_0_norm1 : [num_users=1] = call_module[target=features.3.0.norm1](args = (%features_2_reduction,), kwargs = {})\n",
      "    %features_3_0_attn_relative_position_bias_table : [num_users=1] = get_attr[target=features.3.0.attn.relative_position_bias_table]\n",
      "    %features_3_0_attn_relative_position_index : [num_users=1] = get_attr[target=features.3.0.attn.relative_position_index]\n",
      "    %_get_relative_position_bias_2 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._get_relative_position_bias](args = (%features_3_0_attn_relative_position_bias_table, %features_3_0_attn_relative_position_index, [7, 7]), kwargs = {})\n",
      "    %features_3_0_attn_qkv_weight : [num_users=1] = get_attr[target=features.3.0.attn.qkv.weight]\n",
      "    %features_3_0_attn_proj_weight : [num_users=1] = get_attr[target=features.3.0.attn.proj.weight]\n",
      "    %features_3_0_attn_qkv_bias : [num_users=1] = get_attr[target=features.3.0.attn.qkv.bias]\n",
      "    %features_3_0_attn_proj_bias : [num_users=1] = get_attr[target=features.3.0.attn.proj.bias]\n",
      "    %shifted_window_attention_2 : [num_users=1] = call_function[target=torchvision.models.swin_transformer.shifted_window_attention](args = (%features_3_0_norm1, %features_3_0_attn_qkv_weight, %features_3_0_attn_proj_weight, %_get_relative_position_bias_2, [7, 7], 6), kwargs = {shift_size: [0, 0], attention_dropout: 0.0, dropout: 0.0, qkv_bias: %features_3_0_attn_qkv_bias, proj_bias: %features_3_0_attn_proj_bias, training: True})\n",
      "    %features_3_0_stochastic_depth : [num_users=1] = call_module[target=features.3.0.stochastic_depth](args = (%shifted_window_attention_2,), kwargs = {})\n",
      "    %add_4 : [num_users=2] = call_function[target=operator.add](args = (%features_2_reduction, %features_3_0_stochastic_depth), kwargs = {})\n",
      "    %features_3_0_norm2 : [num_users=1] = call_module[target=features.3.0.norm2](args = (%add_4,), kwargs = {})\n",
      "    %features_3_0_mlp : [num_users=1] = call_module[target=features.3.0.mlp](args = (%features_3_0_norm2,), kwargs = {})\n",
      "    %features_3_0_stochastic_depth_1 : [num_users=1] = call_module[target=features.3.0.stochastic_depth](args = (%features_3_0_mlp,), kwargs = {})\n",
      "    %add_5 : [num_users=2] = call_function[target=operator.add](args = (%add_4, %features_3_0_stochastic_depth_1), kwargs = {})\n",
      "    %features_3_1_norm1 : [num_users=1] = call_module[target=features.3.1.norm1](args = (%add_5,), kwargs = {})\n",
      "    %features_3_1_attn_relative_position_bias_table : [num_users=1] = get_attr[target=features.3.1.attn.relative_position_bias_table]\n",
      "    %features_3_1_attn_relative_position_index : [num_users=1] = get_attr[target=features.3.1.attn.relative_position_index]\n",
      "    %_get_relative_position_bias_3 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._get_relative_position_bias](args = (%features_3_1_attn_relative_position_bias_table, %features_3_1_attn_relative_position_index, [7, 7]), kwargs = {})\n",
      "    %features_3_1_attn_qkv_weight : [num_users=1] = get_attr[target=features.3.1.attn.qkv.weight]\n",
      "    %features_3_1_attn_proj_weight : [num_users=1] = get_attr[target=features.3.1.attn.proj.weight]\n",
      "    %features_3_1_attn_qkv_bias : [num_users=1] = get_attr[target=features.3.1.attn.qkv.bias]\n",
      "    %features_3_1_attn_proj_bias : [num_users=1] = get_attr[target=features.3.1.attn.proj.bias]\n",
      "    %shifted_window_attention_3 : [num_users=1] = call_function[target=torchvision.models.swin_transformer.shifted_window_attention](args = (%features_3_1_norm1, %features_3_1_attn_qkv_weight, %features_3_1_attn_proj_weight, %_get_relative_position_bias_3, [7, 7], 6), kwargs = {shift_size: [3, 3], attention_dropout: 0.0, dropout: 0.0, qkv_bias: %features_3_1_attn_qkv_bias, proj_bias: %features_3_1_attn_proj_bias, training: True})\n",
      "    %features_3_1_stochastic_depth : [num_users=1] = call_module[target=features.3.1.stochastic_depth](args = (%shifted_window_attention_3,), kwargs = {})\n",
      "    %add_6 : [num_users=2] = call_function[target=operator.add](args = (%add_5, %features_3_1_stochastic_depth), kwargs = {})\n",
      "    %features_3_1_norm2 : [num_users=1] = call_module[target=features.3.1.norm2](args = (%add_6,), kwargs = {})\n",
      "    %features_3_1_mlp : [num_users=1] = call_module[target=features.3.1.mlp](args = (%features_3_1_norm2,), kwargs = {})\n",
      "    %features_3_1_stochastic_depth_1 : [num_users=1] = call_module[target=features.3.1.stochastic_depth](args = (%features_3_1_mlp,), kwargs = {})\n",
      "    %add_7 : [num_users=1] = call_function[target=operator.add](args = (%add_6, %features_3_1_stochastic_depth_1), kwargs = {})\n",
      "    %_patch_merging_pad_1 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._patch_merging_pad](args = (%add_7,), kwargs = {})\n",
      "    %features_4_norm : [num_users=1] = call_module[target=features.4.norm](args = (%_patch_merging_pad_1,), kwargs = {})\n",
      "    %features_4_reduction : [num_users=2] = call_module[target=features.4.reduction](args = (%features_4_norm,), kwargs = {})\n",
      "    %features_5_0_norm1 : [num_users=1] = call_module[target=features.5.0.norm1](args = (%features_4_reduction,), kwargs = {})\n",
      "    %features_5_0_attn_relative_position_bias_table : [num_users=1] = get_attr[target=features.5.0.attn.relative_position_bias_table]\n",
      "    %features_5_0_attn_relative_position_index : [num_users=1] = get_attr[target=features.5.0.attn.relative_position_index]\n",
      "    %_get_relative_position_bias_4 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._get_relative_position_bias](args = (%features_5_0_attn_relative_position_bias_table, %features_5_0_attn_relative_position_index, [7, 7]), kwargs = {})\n",
      "    %features_5_0_attn_qkv_weight : [num_users=1] = get_attr[target=features.5.0.attn.qkv.weight]\n",
      "    %features_5_0_attn_proj_weight : [num_users=1] = get_attr[target=features.5.0.attn.proj.weight]\n",
      "    %features_5_0_attn_qkv_bias : [num_users=1] = get_attr[target=features.5.0.attn.qkv.bias]\n",
      "    %features_5_0_attn_proj_bias : [num_users=1] = get_attr[target=features.5.0.attn.proj.bias]\n",
      "    %shifted_window_attention_4 : [num_users=1] = call_function[target=torchvision.models.swin_transformer.shifted_window_attention](args = (%features_5_0_norm1, %features_5_0_attn_qkv_weight, %features_5_0_attn_proj_weight, %_get_relative_position_bias_4, [7, 7], 12), kwargs = {shift_size: [0, 0], attention_dropout: 0.0, dropout: 0.0, qkv_bias: %features_5_0_attn_qkv_bias, proj_bias: %features_5_0_attn_proj_bias, training: True})\n",
      "    %features_5_0_stochastic_depth : [num_users=1] = call_module[target=features.5.0.stochastic_depth](args = (%shifted_window_attention_4,), kwargs = {})\n",
      "    %add_8 : [num_users=2] = call_function[target=operator.add](args = (%features_4_reduction, %features_5_0_stochastic_depth), kwargs = {})\n",
      "    %features_5_0_norm2 : [num_users=1] = call_module[target=features.5.0.norm2](args = (%add_8,), kwargs = {})\n",
      "    %features_5_0_mlp : [num_users=1] = call_module[target=features.5.0.mlp](args = (%features_5_0_norm2,), kwargs = {})\n",
      "    %features_5_0_stochastic_depth_1 : [num_users=1] = call_module[target=features.5.0.stochastic_depth](args = (%features_5_0_mlp,), kwargs = {})\n",
      "    %add_9 : [num_users=2] = call_function[target=operator.add](args = (%add_8, %features_5_0_stochastic_depth_1), kwargs = {})\n",
      "    %features_5_1_norm1 : [num_users=1] = call_module[target=features.5.1.norm1](args = (%add_9,), kwargs = {})\n",
      "    %features_5_1_attn_relative_position_bias_table : [num_users=1] = get_attr[target=features.5.1.attn.relative_position_bias_table]\n",
      "    %features_5_1_attn_relative_position_index : [num_users=1] = get_attr[target=features.5.1.attn.relative_position_index]\n",
      "    %_get_relative_position_bias_5 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._get_relative_position_bias](args = (%features_5_1_attn_relative_position_bias_table, %features_5_1_attn_relative_position_index, [7, 7]), kwargs = {})\n",
      "    %features_5_1_attn_qkv_weight : [num_users=1] = get_attr[target=features.5.1.attn.qkv.weight]\n",
      "    %features_5_1_attn_proj_weight : [num_users=1] = get_attr[target=features.5.1.attn.proj.weight]\n",
      "    %features_5_1_attn_qkv_bias : [num_users=1] = get_attr[target=features.5.1.attn.qkv.bias]\n",
      "    %features_5_1_attn_proj_bias : [num_users=1] = get_attr[target=features.5.1.attn.proj.bias]\n",
      "    %shifted_window_attention_5 : [num_users=1] = call_function[target=torchvision.models.swin_transformer.shifted_window_attention](args = (%features_5_1_norm1, %features_5_1_attn_qkv_weight, %features_5_1_attn_proj_weight, %_get_relative_position_bias_5, [7, 7], 12), kwargs = {shift_size: [3, 3], attention_dropout: 0.0, dropout: 0.0, qkv_bias: %features_5_1_attn_qkv_bias, proj_bias: %features_5_1_attn_proj_bias, training: True})\n",
      "    %features_5_1_stochastic_depth : [num_users=1] = call_module[target=features.5.1.stochastic_depth](args = (%shifted_window_attention_5,), kwargs = {})\n",
      "    %add_10 : [num_users=2] = call_function[target=operator.add](args = (%add_9, %features_5_1_stochastic_depth), kwargs = {})\n",
      "    %features_5_1_norm2 : [num_users=1] = call_module[target=features.5.1.norm2](args = (%add_10,), kwargs = {})\n",
      "    %features_5_1_mlp : [num_users=1] = call_module[target=features.5.1.mlp](args = (%features_5_1_norm2,), kwargs = {})\n",
      "    %features_5_1_stochastic_depth_1 : [num_users=1] = call_module[target=features.5.1.stochastic_depth](args = (%features_5_1_mlp,), kwargs = {})\n",
      "    %add_11 : [num_users=2] = call_function[target=operator.add](args = (%add_10, %features_5_1_stochastic_depth_1), kwargs = {})\n",
      "    %features_5_2_norm1 : [num_users=1] = call_module[target=features.5.2.norm1](args = (%add_11,), kwargs = {})\n",
      "    %features_5_2_attn_relative_position_bias_table : [num_users=1] = get_attr[target=features.5.2.attn.relative_position_bias_table]\n",
      "    %features_5_2_attn_relative_position_index : [num_users=1] = get_attr[target=features.5.2.attn.relative_position_index]\n",
      "    %_get_relative_position_bias_6 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._get_relative_position_bias](args = (%features_5_2_attn_relative_position_bias_table, %features_5_2_attn_relative_position_index, [7, 7]), kwargs = {})\n",
      "    %features_5_2_attn_qkv_weight : [num_users=1] = get_attr[target=features.5.2.attn.qkv.weight]\n",
      "    %features_5_2_attn_proj_weight : [num_users=1] = get_attr[target=features.5.2.attn.proj.weight]\n",
      "    %features_5_2_attn_qkv_bias : [num_users=1] = get_attr[target=features.5.2.attn.qkv.bias]\n",
      "    %features_5_2_attn_proj_bias : [num_users=1] = get_attr[target=features.5.2.attn.proj.bias]\n",
      "    %shifted_window_attention_6 : [num_users=1] = call_function[target=torchvision.models.swin_transformer.shifted_window_attention](args = (%features_5_2_norm1, %features_5_2_attn_qkv_weight, %features_5_2_attn_proj_weight, %_get_relative_position_bias_6, [7, 7], 12), kwargs = {shift_size: [0, 0], attention_dropout: 0.0, dropout: 0.0, qkv_bias: %features_5_2_attn_qkv_bias, proj_bias: %features_5_2_attn_proj_bias, training: True})\n",
      "    %features_5_2_stochastic_depth : [num_users=1] = call_module[target=features.5.2.stochastic_depth](args = (%shifted_window_attention_6,), kwargs = {})\n",
      "    %add_12 : [num_users=2] = call_function[target=operator.add](args = (%add_11, %features_5_2_stochastic_depth), kwargs = {})\n",
      "    %features_5_2_norm2 : [num_users=1] = call_module[target=features.5.2.norm2](args = (%add_12,), kwargs = {})\n",
      "    %features_5_2_mlp : [num_users=1] = call_module[target=features.5.2.mlp](args = (%features_5_2_norm2,), kwargs = {})\n",
      "    %features_5_2_stochastic_depth_1 : [num_users=1] = call_module[target=features.5.2.stochastic_depth](args = (%features_5_2_mlp,), kwargs = {})\n",
      "    %add_13 : [num_users=2] = call_function[target=operator.add](args = (%add_12, %features_5_2_stochastic_depth_1), kwargs = {})\n",
      "    %features_5_3_norm1 : [num_users=1] = call_module[target=features.5.3.norm1](args = (%add_13,), kwargs = {})\n",
      "    %features_5_3_attn_relative_position_bias_table : [num_users=1] = get_attr[target=features.5.3.attn.relative_position_bias_table]\n",
      "    %features_5_3_attn_relative_position_index : [num_users=1] = get_attr[target=features.5.3.attn.relative_position_index]\n",
      "    %_get_relative_position_bias_7 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._get_relative_position_bias](args = (%features_5_3_attn_relative_position_bias_table, %features_5_3_attn_relative_position_index, [7, 7]), kwargs = {})\n",
      "    %features_5_3_attn_qkv_weight : [num_users=1] = get_attr[target=features.5.3.attn.qkv.weight]\n",
      "    %features_5_3_attn_proj_weight : [num_users=1] = get_attr[target=features.5.3.attn.proj.weight]\n",
      "    %features_5_3_attn_qkv_bias : [num_users=1] = get_attr[target=features.5.3.attn.qkv.bias]\n",
      "    %features_5_3_attn_proj_bias : [num_users=1] = get_attr[target=features.5.3.attn.proj.bias]\n",
      "    %shifted_window_attention_7 : [num_users=1] = call_function[target=torchvision.models.swin_transformer.shifted_window_attention](args = (%features_5_3_norm1, %features_5_3_attn_qkv_weight, %features_5_3_attn_proj_weight, %_get_relative_position_bias_7, [7, 7], 12), kwargs = {shift_size: [3, 3], attention_dropout: 0.0, dropout: 0.0, qkv_bias: %features_5_3_attn_qkv_bias, proj_bias: %features_5_3_attn_proj_bias, training: True})\n",
      "    %features_5_3_stochastic_depth : [num_users=1] = call_module[target=features.5.3.stochastic_depth](args = (%shifted_window_attention_7,), kwargs = {})\n",
      "    %add_14 : [num_users=2] = call_function[target=operator.add](args = (%add_13, %features_5_3_stochastic_depth), kwargs = {})\n",
      "    %features_5_3_norm2 : [num_users=1] = call_module[target=features.5.3.norm2](args = (%add_14,), kwargs = {})\n",
      "    %features_5_3_mlp : [num_users=1] = call_module[target=features.5.3.mlp](args = (%features_5_3_norm2,), kwargs = {})\n",
      "    %features_5_3_stochastic_depth_1 : [num_users=1] = call_module[target=features.5.3.stochastic_depth](args = (%features_5_3_mlp,), kwargs = {})\n",
      "    %add_15 : [num_users=2] = call_function[target=operator.add](args = (%add_14, %features_5_3_stochastic_depth_1), kwargs = {})\n",
      "    %features_5_4_norm1 : [num_users=1] = call_module[target=features.5.4.norm1](args = (%add_15,), kwargs = {})\n",
      "    %features_5_4_attn_relative_position_bias_table : [num_users=1] = get_attr[target=features.5.4.attn.relative_position_bias_table]\n",
      "    %features_5_4_attn_relative_position_index : [num_users=1] = get_attr[target=features.5.4.attn.relative_position_index]\n",
      "    %_get_relative_position_bias_8 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._get_relative_position_bias](args = (%features_5_4_attn_relative_position_bias_table, %features_5_4_attn_relative_position_index, [7, 7]), kwargs = {})\n",
      "    %features_5_4_attn_qkv_weight : [num_users=1] = get_attr[target=features.5.4.attn.qkv.weight]\n",
      "    %features_5_4_attn_proj_weight : [num_users=1] = get_attr[target=features.5.4.attn.proj.weight]\n",
      "    %features_5_4_attn_qkv_bias : [num_users=1] = get_attr[target=features.5.4.attn.qkv.bias]\n",
      "    %features_5_4_attn_proj_bias : [num_users=1] = get_attr[target=features.5.4.attn.proj.bias]\n",
      "    %shifted_window_attention_8 : [num_users=1] = call_function[target=torchvision.models.swin_transformer.shifted_window_attention](args = (%features_5_4_norm1, %features_5_4_attn_qkv_weight, %features_5_4_attn_proj_weight, %_get_relative_position_bias_8, [7, 7], 12), kwargs = {shift_size: [0, 0], attention_dropout: 0.0, dropout: 0.0, qkv_bias: %features_5_4_attn_qkv_bias, proj_bias: %features_5_4_attn_proj_bias, training: True})\n",
      "    %features_5_4_stochastic_depth : [num_users=1] = call_module[target=features.5.4.stochastic_depth](args = (%shifted_window_attention_8,), kwargs = {})\n",
      "    %add_16 : [num_users=2] = call_function[target=operator.add](args = (%add_15, %features_5_4_stochastic_depth), kwargs = {})\n",
      "    %features_5_4_norm2 : [num_users=1] = call_module[target=features.5.4.norm2](args = (%add_16,), kwargs = {})\n",
      "    %features_5_4_mlp : [num_users=1] = call_module[target=features.5.4.mlp](args = (%features_5_4_norm2,), kwargs = {})\n",
      "    %features_5_4_stochastic_depth_1 : [num_users=1] = call_module[target=features.5.4.stochastic_depth](args = (%features_5_4_mlp,), kwargs = {})\n",
      "    %add_17 : [num_users=2] = call_function[target=operator.add](args = (%add_16, %features_5_4_stochastic_depth_1), kwargs = {})\n",
      "    %features_5_5_norm1 : [num_users=1] = call_module[target=features.5.5.norm1](args = (%add_17,), kwargs = {})\n",
      "    %features_5_5_attn_relative_position_bias_table : [num_users=1] = get_attr[target=features.5.5.attn.relative_position_bias_table]\n",
      "    %features_5_5_attn_relative_position_index : [num_users=1] = get_attr[target=features.5.5.attn.relative_position_index]\n",
      "    %_get_relative_position_bias_9 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._get_relative_position_bias](args = (%features_5_5_attn_relative_position_bias_table, %features_5_5_attn_relative_position_index, [7, 7]), kwargs = {})\n",
      "    %features_5_5_attn_qkv_weight : [num_users=1] = get_attr[target=features.5.5.attn.qkv.weight]\n",
      "    %features_5_5_attn_proj_weight : [num_users=1] = get_attr[target=features.5.5.attn.proj.weight]\n",
      "    %features_5_5_attn_qkv_bias : [num_users=1] = get_attr[target=features.5.5.attn.qkv.bias]\n",
      "    %features_5_5_attn_proj_bias : [num_users=1] = get_attr[target=features.5.5.attn.proj.bias]\n",
      "    %shifted_window_attention_9 : [num_users=1] = call_function[target=torchvision.models.swin_transformer.shifted_window_attention](args = (%features_5_5_norm1, %features_5_5_attn_qkv_weight, %features_5_5_attn_proj_weight, %_get_relative_position_bias_9, [7, 7], 12), kwargs = {shift_size: [3, 3], attention_dropout: 0.0, dropout: 0.0, qkv_bias: %features_5_5_attn_qkv_bias, proj_bias: %features_5_5_attn_proj_bias, training: True})\n",
      "    %features_5_5_stochastic_depth : [num_users=1] = call_module[target=features.5.5.stochastic_depth](args = (%shifted_window_attention_9,), kwargs = {})\n",
      "    %add_18 : [num_users=2] = call_function[target=operator.add](args = (%add_17, %features_5_5_stochastic_depth), kwargs = {})\n",
      "    %features_5_5_norm2 : [num_users=1] = call_module[target=features.5.5.norm2](args = (%add_18,), kwargs = {})\n",
      "    %features_5_5_mlp : [num_users=1] = call_module[target=features.5.5.mlp](args = (%features_5_5_norm2,), kwargs = {})\n",
      "    %features_5_5_stochastic_depth_1 : [num_users=1] = call_module[target=features.5.5.stochastic_depth](args = (%features_5_5_mlp,), kwargs = {})\n",
      "    %add_19 : [num_users=1] = call_function[target=operator.add](args = (%add_18, %features_5_5_stochastic_depth_1), kwargs = {})\n",
      "    %_patch_merging_pad_2 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._patch_merging_pad](args = (%add_19,), kwargs = {})\n",
      "    %features_6_norm : [num_users=1] = call_module[target=features.6.norm](args = (%_patch_merging_pad_2,), kwargs = {})\n",
      "    %features_6_reduction : [num_users=2] = call_module[target=features.6.reduction](args = (%features_6_norm,), kwargs = {})\n",
      "    %features_7_0_norm1 : [num_users=1] = call_module[target=features.7.0.norm1](args = (%features_6_reduction,), kwargs = {})\n",
      "    %features_7_0_attn_relative_position_bias_table : [num_users=1] = get_attr[target=features.7.0.attn.relative_position_bias_table]\n",
      "    %features_7_0_attn_relative_position_index : [num_users=1] = get_attr[target=features.7.0.attn.relative_position_index]\n",
      "    %_get_relative_position_bias_10 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._get_relative_position_bias](args = (%features_7_0_attn_relative_position_bias_table, %features_7_0_attn_relative_position_index, [7, 7]), kwargs = {})\n",
      "    %features_7_0_attn_qkv_weight : [num_users=1] = get_attr[target=features.7.0.attn.qkv.weight]\n",
      "    %features_7_0_attn_proj_weight : [num_users=1] = get_attr[target=features.7.0.attn.proj.weight]\n",
      "    %features_7_0_attn_qkv_bias : [num_users=1] = get_attr[target=features.7.0.attn.qkv.bias]\n",
      "    %features_7_0_attn_proj_bias : [num_users=1] = get_attr[target=features.7.0.attn.proj.bias]\n",
      "    %shifted_window_attention_10 : [num_users=1] = call_function[target=torchvision.models.swin_transformer.shifted_window_attention](args = (%features_7_0_norm1, %features_7_0_attn_qkv_weight, %features_7_0_attn_proj_weight, %_get_relative_position_bias_10, [7, 7], 24), kwargs = {shift_size: [0, 0], attention_dropout: 0.0, dropout: 0.0, qkv_bias: %features_7_0_attn_qkv_bias, proj_bias: %features_7_0_attn_proj_bias, training: True})\n",
      "    %features_7_0_stochastic_depth : [num_users=1] = call_module[target=features.7.0.stochastic_depth](args = (%shifted_window_attention_10,), kwargs = {})\n",
      "    %add_20 : [num_users=2] = call_function[target=operator.add](args = (%features_6_reduction, %features_7_0_stochastic_depth), kwargs = {})\n",
      "    %features_7_0_norm2 : [num_users=1] = call_module[target=features.7.0.norm2](args = (%add_20,), kwargs = {})\n",
      "    %features_7_0_mlp : [num_users=1] = call_module[target=features.7.0.mlp](args = (%features_7_0_norm2,), kwargs = {})\n",
      "    %features_7_0_stochastic_depth_1 : [num_users=1] = call_module[target=features.7.0.stochastic_depth](args = (%features_7_0_mlp,), kwargs = {})\n",
      "    %add_21 : [num_users=2] = call_function[target=operator.add](args = (%add_20, %features_7_0_stochastic_depth_1), kwargs = {})\n",
      "    %features_7_1_norm1 : [num_users=1] = call_module[target=features.7.1.norm1](args = (%add_21,), kwargs = {})\n",
      "    %features_7_1_attn_relative_position_bias_table : [num_users=1] = get_attr[target=features.7.1.attn.relative_position_bias_table]\n",
      "    %features_7_1_attn_relative_position_index : [num_users=1] = get_attr[target=features.7.1.attn.relative_position_index]\n",
      "    %_get_relative_position_bias_11 : [num_users=1] = call_function[target=torchvision.models.swin_transformer._get_relative_position_bias](args = (%features_7_1_attn_relative_position_bias_table, %features_7_1_attn_relative_position_index, [7, 7]), kwargs = {})\n",
      "    %features_7_1_attn_qkv_weight : [num_users=1] = get_attr[target=features.7.1.attn.qkv.weight]\n",
      "    %features_7_1_attn_proj_weight : [num_users=1] = get_attr[target=features.7.1.attn.proj.weight]\n",
      "    %features_7_1_attn_qkv_bias : [num_users=1] = get_attr[target=features.7.1.attn.qkv.bias]\n",
      "    %features_7_1_attn_proj_bias : [num_users=1] = get_attr[target=features.7.1.attn.proj.bias]\n",
      "    %shifted_window_attention_11 : [num_users=1] = call_function[target=torchvision.models.swin_transformer.shifted_window_attention](args = (%features_7_1_norm1, %features_7_1_attn_qkv_weight, %features_7_1_attn_proj_weight, %_get_relative_position_bias_11, [7, 7], 24), kwargs = {shift_size: [3, 3], attention_dropout: 0.0, dropout: 0.0, qkv_bias: %features_7_1_attn_qkv_bias, proj_bias: %features_7_1_attn_proj_bias, training: True})\n",
      "    %features_7_1_stochastic_depth : [num_users=1] = call_module[target=features.7.1.stochastic_depth](args = (%shifted_window_attention_11,), kwargs = {})\n",
      "    %add_22 : [num_users=2] = call_function[target=operator.add](args = (%add_21, %features_7_1_stochastic_depth), kwargs = {})\n",
      "    %features_7_1_norm2 : [num_users=1] = call_module[target=features.7.1.norm2](args = (%add_22,), kwargs = {})\n",
      "    %features_7_1_mlp : [num_users=1] = call_module[target=features.7.1.mlp](args = (%features_7_1_norm2,), kwargs = {})\n",
      "    %features_7_1_stochastic_depth_1 : [num_users=1] = call_module[target=features.7.1.stochastic_depth](args = (%features_7_1_mlp,), kwargs = {})\n",
      "    %add_23 : [num_users=1] = call_function[target=operator.add](args = (%add_22, %features_7_1_stochastic_depth_1), kwargs = {})\n",
      "    %norm : [num_users=1] = call_module[target=norm](args = (%add_23,), kwargs = {})\n",
      "    %permute : [num_users=1] = call_module[target=permute](args = (%norm,), kwargs = {})\n",
      "    %avgpool : [num_users=1] = call_module[target=avgpool](args = (%permute,), kwargs = {})\n",
      "    %flatten : [num_users=1] = call_module[target=flatten](args = (%avgpool,), kwargs = {})\n",
      "    %head : [num_users=1] = call_module[target=head](args = (%flatten,), kwargs = {})\n",
      "    return head\n"
     ]
    }
   ],
   "source": [
    "print(graph)\n",
    "# graph.print_tabular()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-25T13:42:09.692804Z",
     "start_time": "2025-06-25T13:42:09.648709Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x --- x\n",
      "features.0.0 --- features_0_0\n",
      "features.0.1 --- features_0_1\n",
      "features.0.2 --- features_0_2\n",
      "features.1.0.norm1 --- features_1_0_norm1\n",
      "features.1.0.attn.features_1_0_attn_relative_position_bias_table --- features_1_0_attn_relative_position_bias_table\n",
      "features.1.0.attn._get_relative_position_bias --- _get_relative_position_bias\n",
      "features.1.0.attn.features_1_0_attn_qkv_weight --- features_1_0_attn_qkv_weight\n",
      "features.1.0.attn.features_1_0_attn_proj_weight --- features_1_0_attn_proj_weight\n",
      "features.1.0.attn.features_1_0_attn_qkv_bias --- features_1_0_attn_qkv_bias\n",
      "features.1.0.attn.features_1_0_attn_proj_bias --- features_1_0_attn_proj_bias\n",
      "features.1.0.attn.shifted_window_attention --- shifted_window_attention\n",
      "features.1.0.stochastic_depth --- features_1_0_stochastic_depth\n",
      "features.1.0.add --- add\n",
      "features.1.0.norm2 --- features_1_0_norm2\n",
      "features.1.0.mlp --- features_1_0_mlp\n",
      "features.1.0.stochastic_depth_1 --- features_1_0_stochastic_depth_1\n",
      "features.1.0.add_1 --- add_1\n",
      "features.1.1.norm1 --- features_1_1_norm1\n",
      "features.1.1.attn.features_1_1_attn_relative_position_bias_table --- features_1_1_attn_relative_position_bias_table\n",
      "features.1.1.attn._get_relative_position_bias --- _get_relative_position_bias_1\n",
      "features.1.1.attn.features_1_1_attn_qkv_weight --- features_1_1_attn_qkv_weight\n",
      "features.1.1.attn.features_1_1_attn_proj_weight --- features_1_1_attn_proj_weight\n",
      "features.1.1.attn.features_1_1_attn_qkv_bias --- features_1_1_attn_qkv_bias\n",
      "features.1.1.attn.features_1_1_attn_proj_bias --- features_1_1_attn_proj_bias\n",
      "features.1.1.attn.shifted_window_attention --- shifted_window_attention_1\n",
      "features.1.1.stochastic_depth --- features_1_1_stochastic_depth\n",
      "features.1.1.add --- add_2\n",
      "features.1.1.norm2 --- features_1_1_norm2\n",
      "features.1.1.mlp --- features_1_1_mlp\n",
      "features.1.1.stochastic_depth_1 --- features_1_1_stochastic_depth_1\n",
      "features.1.1.add_1 --- add_3\n",
      "features.2._patch_merging_pad --- _patch_merging_pad\n",
      "features.2.norm --- features_2_norm\n",
      "features.2.reduction --- features_2_reduction\n",
      "features.3.0.norm1 --- features_3_0_norm1\n",
      "features.3.0.attn.features_3_0_attn_relative_position_bias_table --- features_3_0_attn_relative_position_bias_table\n",
      "features.3.0.attn._get_relative_position_bias --- _get_relative_position_bias_2\n",
      "features.3.0.attn.features_3_0_attn_qkv_weight --- features_3_0_attn_qkv_weight\n",
      "features.3.0.attn.features_3_0_attn_proj_weight --- features_3_0_attn_proj_weight\n",
      "features.3.0.attn.features_3_0_attn_qkv_bias --- features_3_0_attn_qkv_bias\n",
      "features.3.0.attn.features_3_0_attn_proj_bias --- features_3_0_attn_proj_bias\n",
      "features.3.0.attn.shifted_window_attention --- shifted_window_attention_2\n",
      "features.3.0.stochastic_depth --- features_3_0_stochastic_depth\n",
      "features.3.0.add --- add_4\n",
      "features.3.0.norm2 --- features_3_0_norm2\n",
      "features.3.0.mlp --- features_3_0_mlp\n",
      "features.3.0.stochastic_depth_1 --- features_3_0_stochastic_depth_1\n",
      "features.3.0.add_1 --- add_5\n",
      "features.3.1.norm1 --- features_3_1_norm1\n",
      "features.3.1.attn.features_3_1_attn_relative_position_bias_table --- features_3_1_attn_relative_position_bias_table\n",
      "features.3.1.attn._get_relative_position_bias --- _get_relative_position_bias_3\n",
      "features.3.1.attn.features_3_1_attn_qkv_weight --- features_3_1_attn_qkv_weight\n",
      "features.3.1.attn.features_3_1_attn_proj_weight --- features_3_1_attn_proj_weight\n",
      "features.3.1.attn.features_3_1_attn_qkv_bias --- features_3_1_attn_qkv_bias\n",
      "features.3.1.attn.features_3_1_attn_proj_bias --- features_3_1_attn_proj_bias\n",
      "features.3.1.attn.shifted_window_attention --- shifted_window_attention_3\n",
      "features.3.1.stochastic_depth --- features_3_1_stochastic_depth\n",
      "features.3.1.add --- add_6\n",
      "features.3.1.norm2 --- features_3_1_norm2\n",
      "features.3.1.mlp --- features_3_1_mlp\n",
      "features.3.1.stochastic_depth_1 --- features_3_1_stochastic_depth_1\n",
      "features.3.1.add_1 --- add_7\n",
      "features.4._patch_merging_pad --- _patch_merging_pad_1\n",
      "features.4.norm --- features_4_norm\n",
      "features.4.reduction --- features_4_reduction\n",
      "features.5.0.norm1 --- features_5_0_norm1\n",
      "features.5.0.attn.features_5_0_attn_relative_position_bias_table --- features_5_0_attn_relative_position_bias_table\n",
      "features.5.0.attn._get_relative_position_bias --- _get_relative_position_bias_4\n",
      "features.5.0.attn.features_5_0_attn_qkv_weight --- features_5_0_attn_qkv_weight\n",
      "features.5.0.attn.features_5_0_attn_proj_weight --- features_5_0_attn_proj_weight\n",
      "features.5.0.attn.features_5_0_attn_qkv_bias --- features_5_0_attn_qkv_bias\n",
      "features.5.0.attn.features_5_0_attn_proj_bias --- features_5_0_attn_proj_bias\n",
      "features.5.0.attn.shifted_window_attention --- shifted_window_attention_4\n",
      "features.5.0.stochastic_depth --- features_5_0_stochastic_depth\n",
      "features.5.0.add --- add_8\n",
      "features.5.0.norm2 --- features_5_0_norm2\n",
      "features.5.0.mlp --- features_5_0_mlp\n",
      "features.5.0.stochastic_depth_1 --- features_5_0_stochastic_depth_1\n",
      "features.5.0.add_1 --- add_9\n",
      "features.5.1.norm1 --- features_5_1_norm1\n",
      "features.5.1.attn.features_5_1_attn_relative_position_bias_table --- features_5_1_attn_relative_position_bias_table\n",
      "features.5.1.attn._get_relative_position_bias --- _get_relative_position_bias_5\n",
      "features.5.1.attn.features_5_1_attn_qkv_weight --- features_5_1_attn_qkv_weight\n",
      "features.5.1.attn.features_5_1_attn_proj_weight --- features_5_1_attn_proj_weight\n",
      "features.5.1.attn.features_5_1_attn_qkv_bias --- features_5_1_attn_qkv_bias\n",
      "features.5.1.attn.features_5_1_attn_proj_bias --- features_5_1_attn_proj_bias\n",
      "features.5.1.attn.shifted_window_attention --- shifted_window_attention_5\n",
      "features.5.1.stochastic_depth --- features_5_1_stochastic_depth\n",
      "features.5.1.add --- add_10\n",
      "features.5.1.norm2 --- features_5_1_norm2\n",
      "features.5.1.mlp --- features_5_1_mlp\n",
      "features.5.1.stochastic_depth_1 --- features_5_1_stochastic_depth_1\n",
      "features.5.1.add_1 --- add_11\n",
      "features.5.2.norm1 --- features_5_2_norm1\n",
      "features.5.2.attn.features_5_2_attn_relative_position_bias_table --- features_5_2_attn_relative_position_bias_table\n",
      "features.5.2.attn._get_relative_position_bias --- _get_relative_position_bias_6\n",
      "features.5.2.attn.features_5_2_attn_qkv_weight --- features_5_2_attn_qkv_weight\n",
      "features.5.2.attn.features_5_2_attn_proj_weight --- features_5_2_attn_proj_weight\n",
      "features.5.2.attn.features_5_2_attn_qkv_bias --- features_5_2_attn_qkv_bias\n",
      "features.5.2.attn.features_5_2_attn_proj_bias --- features_5_2_attn_proj_bias\n",
      "features.5.2.attn.shifted_window_attention --- shifted_window_attention_6\n",
      "features.5.2.stochastic_depth --- features_5_2_stochastic_depth\n",
      "features.5.2.add --- add_12\n",
      "features.5.2.norm2 --- features_5_2_norm2\n",
      "features.5.2.mlp --- features_5_2_mlp\n",
      "features.5.2.stochastic_depth_1 --- features_5_2_stochastic_depth_1\n",
      "features.5.2.add_1 --- add_13\n",
      "features.5.3.norm1 --- features_5_3_norm1\n",
      "features.5.3.attn.features_5_3_attn_relative_position_bias_table --- features_5_3_attn_relative_position_bias_table\n",
      "features.5.3.attn._get_relative_position_bias --- _get_relative_position_bias_7\n",
      "features.5.3.attn.features_5_3_attn_qkv_weight --- features_5_3_attn_qkv_weight\n",
      "features.5.3.attn.features_5_3_attn_proj_weight --- features_5_3_attn_proj_weight\n",
      "features.5.3.attn.features_5_3_attn_qkv_bias --- features_5_3_attn_qkv_bias\n",
      "features.5.3.attn.features_5_3_attn_proj_bias --- features_5_3_attn_proj_bias\n",
      "features.5.3.attn.shifted_window_attention --- shifted_window_attention_7\n",
      "features.5.3.stochastic_depth --- features_5_3_stochastic_depth\n",
      "features.5.3.add --- add_14\n",
      "features.5.3.norm2 --- features_5_3_norm2\n",
      "features.5.3.mlp --- features_5_3_mlp\n",
      "features.5.3.stochastic_depth_1 --- features_5_3_stochastic_depth_1\n",
      "features.5.3.add_1 --- add_15\n",
      "features.5.4.norm1 --- features_5_4_norm1\n",
      "features.5.4.attn.features_5_4_attn_relative_position_bias_table --- features_5_4_attn_relative_position_bias_table\n",
      "features.5.4.attn._get_relative_position_bias --- _get_relative_position_bias_8\n",
      "features.5.4.attn.features_5_4_attn_qkv_weight --- features_5_4_attn_qkv_weight\n",
      "features.5.4.attn.features_5_4_attn_proj_weight --- features_5_4_attn_proj_weight\n",
      "features.5.4.attn.features_5_4_attn_qkv_bias --- features_5_4_attn_qkv_bias\n",
      "features.5.4.attn.features_5_4_attn_proj_bias --- features_5_4_attn_proj_bias\n",
      "features.5.4.attn.shifted_window_attention --- shifted_window_attention_8\n",
      "features.5.4.stochastic_depth --- features_5_4_stochastic_depth\n",
      "features.5.4.add --- add_16\n",
      "features.5.4.norm2 --- features_5_4_norm2\n",
      "features.5.4.mlp --- features_5_4_mlp\n",
      "features.5.4.stochastic_depth_1 --- features_5_4_stochastic_depth_1\n",
      "features.5.4.add_1 --- add_17\n",
      "features.5.5.norm1 --- features_5_5_norm1\n",
      "features.5.5.attn.features_5_5_attn_relative_position_bias_table --- features_5_5_attn_relative_position_bias_table\n",
      "features.5.5.attn._get_relative_position_bias --- _get_relative_position_bias_9\n",
      "features.5.5.attn.features_5_5_attn_qkv_weight --- features_5_5_attn_qkv_weight\n",
      "features.5.5.attn.features_5_5_attn_proj_weight --- features_5_5_attn_proj_weight\n",
      "features.5.5.attn.features_5_5_attn_qkv_bias --- features_5_5_attn_qkv_bias\n",
      "features.5.5.attn.features_5_5_attn_proj_bias --- features_5_5_attn_proj_bias\n",
      "features.5.5.attn.shifted_window_attention --- shifted_window_attention_9\n",
      "features.5.5.stochastic_depth --- features_5_5_stochastic_depth\n",
      "features.5.5.add --- add_18\n",
      "features.5.5.norm2 --- features_5_5_norm2\n",
      "features.5.5.mlp --- features_5_5_mlp\n",
      "features.5.5.stochastic_depth_1 --- features_5_5_stochastic_depth_1\n",
      "features.5.5.add_1 --- add_19\n",
      "features.6._patch_merging_pad --- _patch_merging_pad_2\n",
      "features.6.norm --- features_6_norm\n",
      "features.6.reduction --- features_6_reduction\n",
      "features.7.0.norm1 --- features_7_0_norm1\n",
      "features.7.0.attn.features_7_0_attn_relative_position_bias_table --- features_7_0_attn_relative_position_bias_table\n",
      "features.7.0.attn._get_relative_position_bias --- _get_relative_position_bias_10\n",
      "features.7.0.attn.features_7_0_attn_qkv_weight --- features_7_0_attn_qkv_weight\n",
      "features.7.0.attn.features_7_0_attn_proj_weight --- features_7_0_attn_proj_weight\n",
      "features.7.0.attn.features_7_0_attn_qkv_bias --- features_7_0_attn_qkv_bias\n",
      "features.7.0.attn.features_7_0_attn_proj_bias --- features_7_0_attn_proj_bias\n",
      "features.7.0.attn.shifted_window_attention --- shifted_window_attention_10\n",
      "features.7.0.stochastic_depth --- features_7_0_stochastic_depth\n",
      "features.7.0.add --- add_20\n",
      "features.7.0.norm2 --- features_7_0_norm2\n",
      "features.7.0.mlp --- features_7_0_mlp\n",
      "features.7.0.stochastic_depth_1 --- features_7_0_stochastic_depth_1\n",
      "features.7.0.add_1 --- add_21\n",
      "features.7.1.norm1 --- features_7_1_norm1\n",
      "features.7.1.attn.features_7_1_attn_relative_position_bias_table --- features_7_1_attn_relative_position_bias_table\n",
      "features.7.1.attn._get_relative_position_bias --- _get_relative_position_bias_11\n",
      "features.7.1.attn.features_7_1_attn_qkv_weight --- features_7_1_attn_qkv_weight\n",
      "features.7.1.attn.features_7_1_attn_proj_weight --- features_7_1_attn_proj_weight\n",
      "features.7.1.attn.features_7_1_attn_qkv_bias --- features_7_1_attn_qkv_bias\n",
      "features.7.1.attn.features_7_1_attn_proj_bias --- features_7_1_attn_proj_bias\n",
      "features.7.1.attn.shifted_window_attention --- shifted_window_attention_11\n",
      "features.7.1.stochastic_depth --- features_7_1_stochastic_depth\n",
      "features.7.1.add --- add_22\n",
      "features.7.1.norm2 --- features_7_1_norm2\n",
      "features.7.1.mlp --- features_7_1_mlp\n",
      "features.7.1.stochastic_depth_1 --- features_7_1_stochastic_depth_1\n",
      "features.7.1.add_1 --- add_23\n",
      "norm --- norm\n",
      "permute --- permute\n",
      "avgpool --- avgpool\n",
      "flatten --- flatten\n",
      "head --- head\n"
     ]
    }
   ],
   "source": [
    "for node, qualname in tracer.node_to_qualname.items():\n",
    "    print(f\"{qualname} --- {node}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-25T13:45:38.199141Z",
     "start_time": "2025-06-25T13:45:38.174618Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
