#
# Use ResNet-50 and ViT-Small. There are no stages in ViT so stage boundaries are arbitrary there.
#
# Available models from timm:
#  - resnet50.a1_in1k:                                  80.4% (ResNet Strikes Back A1 recipe)
#  - resnet50.fb_swsl_ig1b_ft_in1k (alt):               81.1% (SWSL Self-Supervised Pre-Training)
#  - vit_small_patch16_224.augreg_in1k:                 78.8% (Supervised method from "How to train your ViT")
#  - vit_small_patch16_224.augreg_in21k_ft_in1k (alt):  81.4% (Self-supervised + fine-tuning from "How to train your ViT")
#

src_stages:
  - Subnet:
      backend: timm
      model_name: resnet50.a1_in1k
      block_input: x
      block_output: layer1.0
      in_format: img  # layer1.0 input is [64, 56, 56].
      out_format: [img, [256, 56, 56]]
  - Subnet:
      backend: timm
      model_name: resnet50.a1_in1k
      block_input: layer1.1
      block_output: layer1.2
      in_format: [img, [256, 56, 56]]
      out_format: [img, [256, 56, 56]]
  - Subnet:  # Downsample block
      backend: timm
      model_name: resnet50.a1_in1k
      block_input: layer2.0
      block_output: layer2.0
      in_format: [img, [256, 56, 56]]
      out_format: [img, [512, 28, 28]]
  - Subnet:
      backend: timm
      model_name: resnet50.a1_in1k
      block_input: layer2.1
      block_output: layer2.3
      in_format: [img, [512, 28, 28]]
      out_format: [img, [512, 28, 28]]
  - Subnet:  # Downsample block
      backend: timm
      model_name: resnet50.a1_in1k
      block_input: layer3.0
      block_output: layer3.0
      in_format: [img, [512, 28, 28]]
      out_format: [img, [1024, 14, 14]]
  - Subnet:
      backend: timm
      model_name: resnet50.a1_in1k
      block_input: layer3.1
      block_output: layer3.5
      in_format: [img, [1024, 14, 14]]
      out_format: [img, [1024, 14, 14]]
  - Subnet:  # Downsample block
      backend: timm
      model_name: resnet50.a1_in1k
      block_input: layer4.0
      block_output: layer4.0
      in_format: [img, [1024, 14, 14]]
      out_format: [img, [2048, 7, 7]]
  - Subnet:
      backend: timm
      model_name: resnet50.a1_in1k
      block_input: layer4.1
      block_output: fc
      in_format: [img, [2048, 7, 7]]
      out_format: vector

dest_stages:
  - Subnet:
      backend: timm
      model_name: vit_small_patch16_224.augreg_in1k
      block_input: x
      block_output: blocks.0
      in_format: img
      out_format: [bert, [384, 196]]
  - Subnet:
      backend: timm
      model_name: vit_small_patch16_224.augreg_in1k
      block_input: blocks.1
      block_output: blocks.2
      in_format: [bert, [384, 196]]
      out_format: [bert, [384, 196]]
  - Subnet:  # Downsample block placeholder (not an actual downsample)
      backend: timm
      model_name: vit_small_patch16_224.augreg_in1k
      block_input: blocks.3
      block_output: blocks.3
      in_format: [bert, [384, 196]]
      out_format: [bert, [384, 196]]
  - Subnet:
      backend: timm
      model_name: vit_small_patch16_224.augreg_in1k
      block_input: blocks.4
      block_output: blocks.5
      in_format: [bert, [384, 196]]
      out_format: [bert, [384, 196]]
  - Subnet:  # Downsample block placeholder (not an actual downsample)
      backend: timm
      model_name: vit_small_patch16_224.augreg_in1k
      block_input: blocks.6
      block_output: blocks.6
      in_format: [bert, [384, 196]]
      out_format: [bert, [384, 196]]
  - Subnet:
      backend: timm
      model_name: vit_small_patch16_224.augreg_in1k
      block_input: blocks.7
      block_output: blocks.8
      in_format: [bert, [384, 196]]
      out_format: [bert, [384, 196]]
  - Subnet:  # Downsample block placeholder (not an actual downsample)
      backend: timm
      model_name: vit_small_patch16_224.augreg_in1k
      block_input: blocks.9
      block_output: blocks.9
      in_format: [bert, [384, 196]]
      out_format: [bert, [384, 196]]
  - Subnet:
      backend: timm
      model_name: vit_small_patch16_224.augreg_in1k
      block_input: blocks.10
      block_output: head
      in_format: [bert, [384, 196]]
      out_format: vector

reformat_options:
  class_token: avg
gaps: !include 4-stage-gaps.yml
stitchers: !include default-stitchers.yml


train_config:
  # Dataset
  dataset: imagenet
  data_augmentation: true

  # General Params
  seed: 12345
  epochs: &epochs 10

  # Optimization
  # In Appendix A.4 of "Revisiting Model Stitching" they say, "All stitching layers were optimized with Adam cosine
  # learning rate schedule and initial learning rate 0.001".
  batch_size: 256
  optimizer: AdamW
  optimizer_args:
    lr: 2.0e-3
    weight_decay: 0.05
  lr_scheduler: CosineAnnealingLR
  lr_scheduler_args:
    T_max: *epochs
    eta_min: 0.0
